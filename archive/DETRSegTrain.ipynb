{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jayan\\anaconda3\\envs\\DTx\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Create dataset class\n",
    "\n",
    "import torch\n",
    "import json\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "class CocoPanoptic(torch.utils.data.Dataset):\n",
    "    def __init__(self, img_folder, ann_folder, ann_file, feature_extractor):\n",
    "        with open(ann_file, 'r') as f:\n",
    "            self.coco = json.load(f)\n",
    "\n",
    "        # sort 'images' field so that they are aligned with 'annotations'\n",
    "        # i.e., in alphabetical order\n",
    "        self.coco['images'] = sorted(self.coco['images'], key=lambda x: x['id'])\n",
    "        # sanity check\n",
    "        if \"annotations\" in self.coco:\n",
    "            for img, ann in zip(self.coco['images'], self.coco['annotations']):\n",
    "                assert img['file_name'][:-4] == ann['file_name'][:-4]\n",
    "\n",
    "        self.img_folder = img_folder\n",
    "        self.ann_folder = Path(ann_folder)\n",
    "        self.ann_file = ann_file\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ann_info = self.coco['annotations'][idx] if \"annotations\" in self.coco else self.coco['images'][idx]\n",
    "        img_path = Path(self.img_folder) / ann_info['file_name'].replace('.png', '.jpg')\n",
    "\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # preprocess image and target (converting target to DETR format, resizing + normalization of both image and target)\n",
    "        encoding = self.feature_extractor(images=img, annotations=ann_info, masks_path=self.ann_folder, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze() # remove batch dimension\n",
    "        target = encoding[\"labels\"][0] # remove batch dimension\n",
    "\n",
    "        return pixel_values, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.coco['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset class using the paths to the images and masks\n",
    "from transformers import DetrFeatureExtractor\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# we reduce the size and max_size to be able to fit the batches in GPU memory\n",
    "feature_extractor = DetrFeatureExtractor.from_pretrained(\"facebook/detr-resnet-50-panoptic\", size=500, max_size=600)\n",
    "\n",
    "def get_folder_paths(subset, device='HPC'):\n",
    "    if device == 'HPC':\n",
    "        root = fr\"../DSAD4DeTr_multilabel/subset/images\"\n",
    "    else:\n",
    "        root = fr\"C:\\Users\\jayan\\Documents\\MECHATRONICS YR4\\MECH5845M - Professional Project\\DSAD4DeTr_multilabel\"\n",
    "    img_folder = os.path.join(root, subset, 'images')\n",
    "    ann_folder = os.path.join(root, subset, 'masks')\n",
    "    ann_file = os.path.join(root, subset, 'annotations', f\"{subset}_panoptic_annotations.json\")\n",
    "    return [img_folder, ann_folder, ann_file]\n",
    "\n",
    "train_paths = get_folder_paths('train', device='CPU')\n",
    "test_paths = get_folder_paths('test', device='CPU')\n",
    "val_paths = get_folder_paths('val', device='CPU')\n",
    "\n",
    "train_dataset = CocoPanoptic(img_folder=train_paths[0], ann_folder=train_paths[1], ann_file=train_paths[2], feature_extractor=feature_extractor)\n",
    "test_dataset = CocoPanoptic(img_folder=test_paths[0], ann_folder=test_paths[1], ann_file=test_paths[2], feature_extractor=feature_extractor)\n",
    "val_dataset = CocoPanoptic(img_folder=val_paths[0], ann_folder=val_paths[1], ann_file=val_paths[2], feature_extractor=feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 419, 599])\n",
      "dict_keys(['size', 'image_id', 'orig_size', 'masks', 'boxes', 'class_labels', 'iscrowd', 'area'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jayan\\anaconda3\\envs\\DTx\\lib\\site-packages\\transformers\\models\\detr\\image_processing_detr.py:882: FutureWarning: The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pixel_values, target = train_dataset[2]\n",
    "print(pixel_values.shape)\n",
    "print(target.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 863\n",
      "Number of test examples: 365\n",
      "Number of validation examples: 202\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of test examples:\", len(test_dataset))\n",
    "print(\"Number of validation examples:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom collate function to batch images and labels together\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "  pixel_values = [item[0] for item in batch]\n",
    "  encoded_input = feature_extractor.pad_and_create_pixel_mask(pixel_values, return_tensors=\"pt\")\n",
    "  labels = [item[1] for item in batch]\n",
    "  batch = {}\n",
    "  batch['pixel_values'] = encoded_input['pixel_values']\n",
    "  batch['pixel_mask'] = encoded_input['pixel_mask']\n",
    "  batch['labels'] = labels\n",
    "  return batch\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=3, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model configuration\n",
    "# Define training configuration\n",
    "n_devices      = 1\n",
    "epochs         = 100 # 20000 is maximum value but will be prevented by early stopping\n",
    "weight_decay   = 1e-4\n",
    "learning_rate  = 1e-4\n",
    "learning_rate_backbone = 1e-5\n",
    "check_val_every_n_epoch = 5\n",
    "load_from_checkpoint = False\n",
    "checkpoint_path = None\n",
    "last_manual_checkpoint = 1\n",
    "last_epoch = 0\n",
    "## LEARNING_SCHEDULER parameters for ReduceLROnPlateau from pytorch\n",
    "factor          = 1e-1\n",
    "lr_patience        = 10\n",
    "lr_delta           = 1e-5\n",
    "lr_monitored_var   = \"training_loss\"\n",
    "min_lr          = 1e-8\n",
    "cooldown        = 5\n",
    "### Parameters for FixedStep from pytorch\n",
    "fix_step   = False\n",
    "step_size  = 60\n",
    "## STOP_CRITERIA parameters for EarlyStopping from pytorch_lightning\n",
    "stop_monitored_var    = \"validation_loss\"\n",
    "stop_delta            = 1e-5\n",
    "mode             = \"min\"\n",
    "stop_patience         = 10   ## Real_patient = patience * check_val_every_n_epoch\n",
    "## Custom loss function\n",
    "loss_tags = None\n",
    "loss_components = []\n",
    "loss_weights = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "\n",
    "class DetrPanoptic(pl.LightningModule):\n",
    "    def __init__(self, model, lr, lr_backbone, weight_decay):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.model = model\n",
    "\n",
    "        # see https://github.com/PyTorchLightning/pytorch-lightning/pull/1896\n",
    "        self.lr = lr\n",
    "        self.lr_backbone = lr_backbone\n",
    "        self.weight_decay = weight_decay\n",
    "    \n",
    "    def forward(self, pixel_values, pixel_mask):\n",
    "        outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def common_step(self, batch, batch_idx):\n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        pixel_mask = batch[\"pixel_mask\"]\n",
    "        labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\n",
    "        outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss_dict = outputs.loss_dict\n",
    "\n",
    "        return loss, loss_dict\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch\n",
    "        self.log(\"training_loss\", loss)\n",
    "        for k,v in loss_dict.items():\n",
    "            self.log(\"train_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)\n",
    "        self.log(\"validation_loss\", loss)\n",
    "        for k,v in loss_dict.items():\n",
    "            self.log(\"validation_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param_dicts = [\n",
    "            {\"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "            {\n",
    "                \"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "                \"lr\": self.lr_backbone,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(param_dicts, lr=self.lr,\n",
    "                                    weight_decay=self.weight_decay)\n",
    "        \n",
    "        learning_rate_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                    factor=factor, patience=lr_patience, threshold=lr_delta,\n",
    "                                    cooldown=cooldown, min_lr=min_lr, verbose=True)\n",
    "\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': learning_rate_scheduler,\n",
    "                'monitor': lr_monitored_var\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DetrForSegmentation were not initialized from the model checkpoint at facebook/detr-resnet-50-panoptic and are newly initialized because the shapes did not match:\n",
      "- detr.class_labels_classifier.weight: found shape torch.Size([251, 256]) in the checkpoint and torch.Size([12, 256]) in the model instantiated\n",
      "- detr.class_labels_classifier.bias: found shape torch.Size([251]) in the checkpoint and torch.Size([12]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['detr.class_labels_classifier.weight', 'detr.class_labels_classifier.bias', 'detr.bbox_predictor.layers.0.weight', 'detr.bbox_predictor.layers.0.bias', 'detr.bbox_predictor.layers.1.weight', 'detr.bbox_predictor.layers.1.bias', 'detr.bbox_predictor.layers.2.weight', 'detr.bbox_predictor.layers.2.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise model\n",
    "from transformers import DetrConfig, DetrForSegmentation\n",
    "\n",
    "model = DetrForSegmentation.from_pretrained(\"facebook/detr-resnet-50-panoptic\", num_labels=11,\n",
    "                                            ignore_mismatched_sizes=True)\n",
    "state_dict = model.state_dict()\n",
    "# For visualisation of initialised layers\n",
    "# for name, param in state_dict.items():\n",
    "#     print(name, param.shape)\n",
    "\n",
    "# Remove class weights\n",
    "del state_dict[\"detr.class_labels_classifier.weight\"]\n",
    "del state_dict[\"detr.class_labels_classifier.bias\"]\n",
    "del state_dict[\"detr.bbox_predictor.layers.0.weight\"]\n",
    "del state_dict[\"detr.bbox_predictor.layers.0.bias\"]\n",
    "del state_dict[\"detr.bbox_predictor.layers.1.weight\"]\n",
    "del state_dict[\"detr.bbox_predictor.layers.1.bias\"]\n",
    "del state_dict[\"detr.bbox_predictor.layers.2.weight\"]\n",
    "del state_dict[\"detr.bbox_predictor.layers.2.bias\"]\n",
    "# define new model with custom class classifier\n",
    "# config = DetrConfig.from_pretrained(\"facebook/detr-resnet-50-panoptic\", num_labels=11)\n",
    "model.load_state_dict(state_dict, strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jayan\\anaconda3\\envs\\DTx\\lib\\site-packages\\transformers\\models\\detr\\image_processing_detr.py:882: FutureWarning: The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jayan\\anaconda3\\envs\\DTx\\lib\\site-packages\\transformers\\models\\detr\\image_processing_detr.py:966: FutureWarning: This method is deprecated and will be removed in v4.27.0. Please use pad instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define LightningModule and verify outputs on single batch\n",
    "model = DetrPanoptic(model=model, lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)\n",
    "\n",
    "# pick the first training batch\n",
    "batch = next(iter(train_dataloader))\n",
    "# forward through the model\n",
    "outputs = model(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of pixel_values: torch.Size([3, 419, 599])\n",
      "Shape of logits: torch.Size([3, 100, 12])\n",
      "Shape of predicted bounding boxes: torch.Size([3, 100, 4])\n",
      "Shape of predicted masks: torch.Size([3, 100, 105, 150])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of pixel_values:\", pixel_values.shape)\n",
    "print(\"Shape of logits:\", outputs.logits.shape)\n",
    "print(\"Shape of predicted bounding boxes:\", outputs.pred_boxes.shape)\n",
    "print(\"Shape of predicted masks:\", outputs.pred_masks.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DTx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
